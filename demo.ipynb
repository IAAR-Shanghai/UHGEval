{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "This demo shows how to perform flexible and customizable evaluation tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.benchs import ExampleQAEvaluator, get_all_evaluator_classes, load_evaluator\n",
    "from eval.llms import HuggingFace, OpenAIAPI\n",
    "from eval.utils import save_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o = OpenAIAPI(model_name=\"gpt4o\", api_key=\"your_api_key\")\n",
    "\n",
    "api_model = OpenAIAPI(\n",
    "    model_name=\"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    api_key=\"your_api_key\",\n",
    "    base_url=\"https://api.siliconflow.cn/v1\",\n",
    ")  # Note: SiliconFlow provides free API keys for many models.\n",
    "\n",
    "hf_model = HuggingFace(\n",
    "    model_name_or_path=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    apply_chat_template=True,\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluator Loading and Evaluation\n",
    "\n",
    "All available evaluators can be found in the [README.md](README.md#eval-suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Directly Use The Evaluator Class\n",
    "evaluator = ExampleQAEvaluator(api_model, num_batches=2)\n",
    "evaluator.evaluate()\n",
    "\n",
    "# Method 2: Load The Evaluator Class By Its Name\n",
    "evaluator_class = load_evaluator(\"ExampleQAEvaluator\")\n",
    "evaluator_from_str = evaluator_class(api_model)\n",
    "evaluator_from_str.evaluate()\n",
    "\n",
    "# Method 3: Iterate All Evaluator Classes To Batch Evaluate\n",
    "# Note: May take a long time to finish.\n",
    "# Comment out the following code if you don't want to run all evaluators.\n",
    "for evaluator_class in get_all_evaluator_classes():\n",
    "    evaluator_item = evaluator_class(api_model)\n",
    "    evaluator_item.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get a CSV file with the evaluation results\n",
    "\n",
    "After evaluation, results are saved at `output` folder by default. You can get an overall statistics of those evaluation results with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_stats()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
